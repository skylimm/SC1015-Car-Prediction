{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we have effectively identified and processed relevant features from the dataset, and built an highly-accurate machine learning model that could accurately predict car prices based on trained and tested data. To select our best model, we have used a variety of metrics such as R-squared (R²) score, Mean Squared Error (MSE), Root Mean Squared Error (RMSE) to measure each model's effectiveness in predicting car price.\n",
    "\n",
    "As the Gradient Boosting Regressor model performed the best in our final train & test dataset, we can conclude that the Gradient Regressor Booster model is our best model in terms of its Goodness of Fit and ability to predict car price, with a final R² of 0.9456 (train) and 0.8984 (test).\n",
    "\n",
    "In addition to successfully building a model, we have also demonstrated how different regression models can be used to train our dataset and how we utilise GridSearchCV for hyperparameter tuning to maximise different model's performance.\n",
    "\n",
    "We also found that different models will have different prediction processes, and hence will use different features to make decisions. For example, our baseline Linear Regression does not include any regularization term, the Lasso and Ridge Regression models that placed more emphasis on regularization parameter alpha and lastly, Gradient Booster Regressor that focus on n_estimators, learning_rate and max_depth to optimize model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation and Possible Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limitation\n",
    "Each machine learning model has its own strengths and limitations. For our chosen model (Gradient Boosting Regressor) in this context, there are mainly 2 limitations: expensive computational cost, sensitivity to hyperparameters and overfitting.\n",
    "\n",
    "One of the limitations of our chosen model (Gradient Boosting Regressor) is that it is computationally expensive, even for our relatively small dataset, it took a significant amount of time to train and cost a large amount of computational power to compute. Extending our model to a larger car sales dataset may result in longer training times apart from increased computational costs.\n",
    "\n",
    "The performance of gradient boosting models is highly dependent on the values of hyperparameters such as the learning rate, number of trees, and maximum depth of the trees. Following from the previous limitation, finding the optimal values of learning rate hyperparameters is not only challenging, but also consumes a significant amount of times and computational resources. \n",
    "\n",
    "Another limitation is overfitting. Overfitting occurs when a model is too complex and has learned the training data too well, leading to poor generalization performance on new, unseen data. This can happen when the model is too flexible, and is able to fit the noise in the training data, rather than the underlying signal. As a result, the model performs very well on the training data, but poorly on the test data. As observed from the above table, the R² value for our test dataset (0.89) is slightly lower than our train dataset (0.94), which suggest that our model is overfitting to the training data, and is unable to generalise as well to new data.\n",
    "\n",
    "##### Improvement\n",
    "We can apply L1 and L2 regularisation techniques to our model to prevent overfitting. Such techniques add penalty terms to the loss function to prevent overfitting. This encourages the model to find the simplest solution that fits the data well, by discouraging the model from fitting to the noise of the data. \n",
    "\n",
    "We can also combine our model with ensemble methods. Ensemble methods include Bagging (short for bootstrap aggregating) and stacking. \n",
    "\n",
    "Bagging involves randomly selecting subsets of training data and then training the model on each subset. The predictions of each model is then collected and combined to make a final prediction. By training multiple models using different subsets of data, bagging can reduce the impact of overfitting.\n",
    "\n",
    "Stacking is similar to bagging but uses the predictions from each sub model as inputs a meta-model. The meta-model is then trained to combine these predictions to make the final prediction. \n",
    "\n",
    "By using predictions of multiple models, ensemble methods can reduce overfitting of our current model and improve our model's prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
